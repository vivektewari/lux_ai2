defaults:
- override hydra/job_logging: colorlog
- override hydra/hydra_logging: colorlog

hydra:
  run:
    dir: ./outputs/${now:%m-%d}/${now:%H-%M-%S}

name: null
## WANDB params
# The wandb project name
project: Lux_AI_2021
# The wandb user to log to
entity: vivektewari2000
# The wandb group for the run
group: na

## ENV params
act_space: BasicActionSpace
obs_space: FixedShapeContinuousObsV2
obs_space_kwargs: {}
reward_space: vivek_mix_reward  #vivek_mix_reward #GetNResearchPoints #CityTileReward #GameResultReward #
reward_space_kwargs: {}
#for#internal calculation |not used anywhere
unit_action_count: 19 #1|0 is no action +4|moves +1|pilage+1|build_city+ 3*4|full transfer
cart_action_count: 17 #1|0 is no action +4|moves + 3*4|transfer
city_action_count: 4 #1|0 th is no action
last_layer: [12*12*(19+17+4)+1,5761] #board area*num action each square+1for state valution|mentiond in units of final unit
## TRAINING params
pretrained: '/home/pooja/PycharmProjects/lux_ai/outputs/monobeast_output/rl_model1_599.pth'
total_steps: 2000000
num_actors: 2
n_actor_envs: 16
unroll_length: 16
batch_size: 1
discounting: 0.999
num_buffers: None
num_learner_threads: 2
## MODEL params
model_arch: FeatureExtractor
nn_model_name: FeatureExtractor
input_dim: [1,53,12,12]
#model_params:
#  channels : [1]
#  input_image_dim : [12, 12]
#  start_channel : 53
#  convs : [3 ,3,3,3,2]
#  pads : [0, 0,0,0,0]
#  strides : [1,1, 1,1,1]
#  pools : [2, 2,1,2,1] #receptive 6,
#  fc1_p : [1,5761]
model_params:
  channels : [1]
  input_image_dim : [12, 12]
  start_channel : 53
  convs : [11,3,4,3,2]
  pads : [0, 0,0,0,0]
  strides : [1,1, 1,1,1]
  pools : [2, 2,1,2,1] #receptive 6,
  fc1_p : [1,5761]
n_blocks: 24
hidden_dim: 128
embedding_dim: 32
n_merge_layers: 1
normalize: False
sum_player_embeddings: False
use_index_select: False
rescale_value_input: True
rescale_se_input: True
# Conv-specific params
kernel_size: 5


## OPTIMIZER params
optimizer_class: Adam
optimizer_kwargs:
  lr: 5e-5
  # See https://arxiv.org/pdf/2105.05246.pdf
  eps: 0.0003
  #alpha: 0.9
min_lr_mod: 0.01

## LOSS params
entropy_cost: 0.0002
baseline_cost: 1.
teacher_kl_cost: 0.005
teacher_baseline_cost: 0.0
# lambda parameter for TD-lambda and UPGO losses
lmb: 0.9
reduction: sum

# Pretrained model for KL loss
use_teacher: False
teacher_load_dir: /Windows/Users/isaia/Documents/GitHub/Kaggle/Lux_AI_2021/outputs/10-10/11-18-12/
teacher_checkpoint_file: 28576448.pt

# MISCELLANEOUS params
learning_rate: 0.01
momentum: 0.01
epsilon: 0.01
alpha: 0.01
actor_device: cuda:1
learner_device: cuda:0
model_log_freq: 100
# file_descriptor or file_system
sharing_strategy: file_descriptor
disable_wandb: False
debug: False


#monobeast paprams
mode        : 'train'
xpid: None
savedir: /home/pooja/PycharmProjects/lux_ai/outputs/monobeast_output/
renderdir: /home/pooja/PycharmProjects/lux_ai/outputs/render/

# Continue from previous run
#load_dir: /Windows/Users/isaia/Documents/GitHub/Kaggle/Lux_AI_2021/outputs/09-07/01-44-10/
#checkpoint_file: 10213056.pt
#weights_only: True
#n_value_warmup_batches: 0